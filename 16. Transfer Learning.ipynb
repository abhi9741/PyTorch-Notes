{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import time\n",
    "import os \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485,0.456,0.406]) #how to get these? similar to fit in keras?\n",
    "std = np.array([0.299,0.224,0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {'train':transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                  transforms.RandomHorizontalFlip(),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean,std)]) , \n",
    "                  'val': transforms.Compose([transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean,std)]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "\n",
    "data_dir = \"data/hymenoptera_data/\"\n",
    "splits = ['train','val']\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in ['train','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset ImageFolder\n",
       "     Number of datapoints: 244\n",
       "     Root location: data/hymenoptera_data/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485 0.456 0.406], std=[0.299 0.224 0.225])\n",
       "            ),\n",
       " 'val': Dataset ImageFolder\n",
       "     Number of datapoints: 153\n",
       "     Root location: data/hymenoptera_data/val\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485 0.456 0.406], std=[0.299 0.224 0.225])\n",
       "            )}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset_loaders = {x: DataLoader(dataset=image_datasets[x],batch_size=batch_size,shuffle=True,num_workers=0) for x in ['train','val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x22c96103b50>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x22c96103a90>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 244, 'val': 153}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "datasets_classes = image_datasets['train'].classes\n",
    "num_classes = len(datasets_classes)\n",
    "\n",
    "print(\"classes:\",datasets_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,loss_func,optimiser,learningrate_scheduler,num_epochs=20):\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0 \n",
    "\n",
    "    steps_per_epoch = {x:len(dataset_loaders['train']) for x in ['train','val']}\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print(\"_\"*14)\n",
    "\n",
    "        #Each epoch has training phase and validation phase\n",
    "        for phase in ['train','val']:\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct_preds = 0\n",
    "\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train() #setting model to train mode\n",
    "\n",
    "                for i,(inputs,labels) in enumerate(dataset_loaders[phase]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    #forward pass\n",
    "                    y_preds = model(inputs)\n",
    "                    _,pred_labels = torch.max(y_preds,1)\n",
    "\n",
    "                    loss = loss_func(y_preds,labels)\n",
    "\n",
    "                    correct_preds = torch.eq(pred_labels,labels).sum().item()\n",
    "\n",
    "                    total_loss += loss\n",
    "                    total_correct_preds += correct_preds\n",
    "\n",
    "                    #backward pass\n",
    "                    optimiser.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    #weight updation\n",
    "                    optimiser.step()\n",
    "                    \n",
    "                 \n",
    "                learningrate_scheduler.step() #changing learning rate after every epoch   \n",
    "                train_epoch_loss = total_loss/len(dataset_loaders[phase])\n",
    "                train_epoch_acc = 100.0*total_correct_preds/dataset_sizes[phase]\n",
    "                print(f\"Epoch {epoch}, Training Loss {train_epoch_loss},Training Acc{train_epoch_acc}\")    \n",
    "\n",
    "            else :\n",
    "                model.eval() #setting model to evaluation mode\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for i,(inputs,labels) in enumerate(dataset_loaders[phase]):\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        #forward pass\n",
    "                        y_preds = model(inputs)\n",
    "                        _,pred_labels = torch.max(y_preds,1)\n",
    "\n",
    "                        loss = loss_func(y_preds,labels)\n",
    "\n",
    "                        correct_preds = torch.eq(pred_labels,labels).sum().item()\n",
    "\n",
    "                        total_loss += loss\n",
    "                        total_correct_preds += correct_preds\n",
    "\n",
    "                    val_epoch_loss = total_loss/len(dataset_loaders[phase])\n",
    "                    val_epoch_acc = 100.0*total_correct_preds/dataset_sizes[phase]\n",
    "                    print(f\"Epoch {epoch}, Val Loss {val_epoch_loss},Val Acc{val_epoch_acc}\")\n",
    "                    #Save the best model\n",
    "                    if val_epoch_acc>best_acc :\n",
    "                        best_acc = val_epoch_acc\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(\"Model at epoch {epoch} saved\")\n",
    "                    \n",
    "            \n",
    "            \n",
    "    print(\"Loading the best model state\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel1(model,loss_func,optimiser,scheduler,num_epochs):\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(\"_\"*20)\n",
    "\n",
    "        #training phase\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i,(inputs,labels) in dataset_loaders['train']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = inputs.to(device)\n",
    "\n",
    "            y_preds = model(inputs)\n",
    "            _,labels_preds = torch.max(y_preds,1)\n",
    "            loss = loss_func(y_preds,labels)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct_preds += torch.eq(labels_preds,labels).sum().item()\n",
    "\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss/len(dataset_loaders['train'])\n",
    "        epoch_acc = 100.0*total_correct_preds/dataset_sizes['train']\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "        #validation phase\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i,(inputs,labels) in dataset_loaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = inputs.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(inputs)\n",
    "                _,labels_preds = torch.max(y_preds,1)\n",
    "                loss = loss_func(y_preds,labels)\n",
    "\n",
    "            \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct_preds += torch.eq(labels_preds,labels).sum().item()\n",
    "\n",
    "\n",
    "        epoch_loss = total_loss/len(dataset_loaders['val'])\n",
    "        epoch_acc = 100.0*total_correct_preds/dataset_sizes['val']\n",
    "\n",
    "        print(f\"Val Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "\n",
    "        if epoch_acc>best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True) #loading the pretrained model\n",
    "#change the last FC layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "#learning schedulers\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimiser,step_size=7,gamma=0.1)\n",
    "#every 7 epochs, learning rate is multiplied by 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/21\n",
      "______________\n",
      "Epoch 0, Training Loss 0.871651291847229,Training Acc34.83606557377049\n",
      "Epoch 0, Val Loss 1.0224231481552124,Val Acc29.41176470588235\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 1/21\n",
      "______________\n",
      "Epoch 1, Training Loss 0.8630949258804321,Training Acc37.295081967213115\n",
      "Epoch 1, Val Loss 1.071572184562683,Val Acc28.104575163398692\n",
      "Epoch 2/21\n",
      "______________\n",
      "Epoch 2, Training Loss 0.875136137008667,Training Acc34.01639344262295\n",
      "Epoch 2, Val Loss 1.0724542140960693,Val Acc25.49019607843137\n",
      "Epoch 3/21\n",
      "______________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\16. Transfer Learning.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000018?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m trainModel(model,loss_func,optimiser, step_lr_scheduler,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m22\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\16. Transfer Learning.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(model, loss_func, optimiser, learningrate_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=21'>22</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain() \u001b[39m#setting model to train mode\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=23'>24</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i,(inputs,labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataset_loaders[phase]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=24'>25</a>\u001b[0m         inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=25'>26</a>\u001b[0m         labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    520\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 521\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    522\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    524\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    525\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    560\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 561\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    562\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    563\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    232\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader(path)\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(sample)\n\u001b[0;32m    235\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    236\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     60\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 61\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\transforms.py:226\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, tensor: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[39m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnormalize(tensor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmean, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstd, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torchvision\\transforms\\functional.py:343\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    340\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mclone()\n\u001b[0;32m    342\u001b[0m dtype \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39mdtype\n\u001b[1;32m--> 343\u001b[0m mean \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mas_tensor(mean, dtype\u001b[39m=\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mtensor\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    344\u001b[0m std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(std, dtype\u001b[39m=\u001b[39mdtype, device\u001b[39m=\u001b[39mtensor\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    345\u001b[0m \u001b[39mif\u001b[39;00m (std \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39many():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trained_model = trainModel(model,loss_func,optimiser, step_lr_scheduler,num_epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing convolutional blocks, only train the fully connected layer\n",
    "\n",
    "model = models.resnet18(pretrained=True) #loading the pretrained model\n",
    "for param in model.parameters(): #freezing the layers\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#change the last FC layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/21\n",
      "______________\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'SGD' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\16. Transfer Learning.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000021?line=0'>1</a>\u001b[0m trained_model \u001b[39m=\u001b[39m trainModel(model,optimiser, step_lr_scheduler,loss_func,num_epochs\u001b[39m=\u001b[39;49m\u001b[39m22\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\16. Transfer Learning.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrainModel\u001b[1;34m(model, loss_func, optimiser, learningrate_scheduler, num_epochs)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=28'>29</a>\u001b[0m y_preds \u001b[39m=\u001b[39m model(inputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=29'>30</a>\u001b[0m _,pred_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(y_preds,\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=31'>32</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(y_preds,labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=33'>34</a>\u001b[0m correct_preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39meq(pred_labels,labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/16.%20Transfer%20Learning.ipynb#ch0000012?line=35'>36</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;31mTypeError\u001b[0m: 'SGD' object is not callable"
     ]
    }
   ],
   "source": [
    "trained_model = trainModel(model,optimiser, step_lr_scheduler,loss_func,num_epochs=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
