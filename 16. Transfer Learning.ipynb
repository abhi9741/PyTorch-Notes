{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import time\n",
    "import os \n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485,0.456,0.406]) #how to get these? similar to fit in keras?\n",
    "std = np.array([0.299,0.224,0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {'train':transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                  transforms.RandomHorizontalFlip(),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean,std)]) , \n",
    "                  'val': transforms.Compose([transforms.Resize(256),\n",
    "                  transforms.CenterCrop(224),\n",
    "                  transforms.ToTensor(),\n",
    "                  transforms.Normalize(mean,std)]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "\n",
    "data_dir = \"data/hymenoptera_data/\"\n",
    "splits = ['train','val']\n",
    "\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir,x),data_transforms[x]) for x in ['train','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset ImageFolder\n",
       "     Number of datapoints: 244\n",
       "     Root location: data/hymenoptera_data/train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485 0.456 0.406], std=[0.299 0.224 0.225])\n",
       "            ),\n",
       " 'val': Dataset ImageFolder\n",
       "     Number of datapoints: 153\n",
       "     Root location: data/hymenoptera_data/val\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
       "                CenterCrop(size=(224, 224))\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.485 0.456 0.406], std=[0.299 0.224 0.225])\n",
       "            )}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "dataset_loaders = {x: DataLoader(dataset=image_datasets[x],batch_size=batch_size,shuffle=True,num_workers=0) for x in ['train','val']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x1dceb31fd90>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x1dcf3a559a0>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train','val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 244, 'val': 153}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes: ['ants', 'bees']\n"
     ]
    }
   ],
   "source": [
    "datasets_classes = image_datasets['train'].classes\n",
    "num_classes = len(datasets_classes)\n",
    "\n",
    "print(\"classes:\",datasets_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model,loss_func,optimiser,learningrate_scheduler,num_epochs=20):\n",
    "    start_time = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0 \n",
    "\n",
    "    steps_per_epoch = {x:len(dataset_loaders['train']) for x in ['train','val']}\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print(\"_\"*14)\n",
    "\n",
    "        #Each epoch has training phase and validation phase\n",
    "        for phase in ['train','val']:\n",
    "\n",
    "            total_loss = 0\n",
    "            total_correct_preds = 0\n",
    "\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train() #setting model to train mode\n",
    "\n",
    "                for i,(inputs,labels) in enumerate(dataset_loaders[phase]):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    #forward pass\n",
    "                    y_preds = model(inputs)\n",
    "                    _,pred_labels = torch.max(y_preds,1)\n",
    "\n",
    "                    loss = loss_func(y_preds,labels)\n",
    "\n",
    "                    correct_preds = torch.eq(pred_labels,labels).sum().item()\n",
    "\n",
    "                    total_loss += loss\n",
    "                    total_correct_preds += correct_preds\n",
    "\n",
    "                    #backward pass\n",
    "                    optimiser.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    #weight updation\n",
    "                    optimiser.step()\n",
    "                    \n",
    "                 \n",
    "                learningrate_scheduler.step() #changing learning rate after every epoch   \n",
    "                train_epoch_loss = total_loss/len(dataset_loaders[phase])\n",
    "                train_epoch_acc = 100.0*total_correct_preds/dataset_sizes[phase]\n",
    "                print(f\"Epoch {epoch}, Training Loss {train_epoch_loss},Training Acc{train_epoch_acc}\")    \n",
    "\n",
    "            else :\n",
    "                model.eval() #setting model to evaluation mode\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    for i,(inputs,labels) in enumerate(dataset_loaders[phase]):\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        #forward pass\n",
    "                        y_preds = model(inputs)\n",
    "                        _,pred_labels = torch.max(y_preds,1)\n",
    "\n",
    "                        loss = loss_func(y_preds,labels)\n",
    "\n",
    "                        correct_preds = torch.eq(pred_labels,labels).sum().item()\n",
    "\n",
    "                        total_loss += loss\n",
    "                        total_correct_preds += correct_preds\n",
    "\n",
    "                    val_epoch_loss = total_loss/len(dataset_loaders[phase])\n",
    "                    val_epoch_acc = 100.0*total_correct_preds/dataset_sizes[phase]\n",
    "                    print(f\"Epoch {epoch}, Val Loss {val_epoch_loss},Val Acc{val_epoch_acc}\")\n",
    "                    #Save the best model\n",
    "                    if val_epoch_acc>best_acc :\n",
    "                        best_acc = val_epoch_acc\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        print(\"Model at epoch {epoch} saved\")\n",
    "                    \n",
    "            \n",
    "            \n",
    "    print(\"Loading the best model state\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel1(model,loss_func,optimiser,scheduler,num_epochs):\n",
    "    best_acc = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(\"_\"*20)\n",
    "\n",
    "        #training phase\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i,(inputs,labels) in enumerate(dataset_loaders['train']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            y_preds = model(inputs)\n",
    "            # print(\"y preds\",y_preds.shape)\n",
    "            _,labels_preds = torch.max(y_preds,1)\n",
    "            # print(\"labels\",labels.shape)\n",
    "            loss = loss_func(y_preds,labels)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct_preds += torch.eq(labels_preds,labels).sum().item()\n",
    "\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss = total_loss/len(dataset_loaders['train'])\n",
    "        epoch_acc = 100.0*total_correct_preds/dataset_sizes['train']\n",
    "\n",
    "        print(f\"Train Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "        #validation phase\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct_preds = 0\n",
    "\n",
    "        for i,(inputs,labels) in enumerate(dataset_loaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(inputs)\n",
    "                _,labels_preds = torch.max(y_preds,1)\n",
    "                loss = loss_func(y_preds,labels)\n",
    "\n",
    "            \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_correct_preds += torch.eq(labels_preds,labels).sum().item()\n",
    "\n",
    "\n",
    "        epoch_loss = total_loss/len(dataset_loaders['val'])\n",
    "        epoch_acc = 100.0*total_correct_preds/dataset_sizes['val']\n",
    "\n",
    "        print(f\"Val Loss: {epoch_loss}, Acc: {epoch_acc}\")\n",
    "\n",
    "\n",
    "        if epoch_acc>best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel2(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataset_loaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing convolutional blocks, only train the fully connected layer\n",
    "\n",
    "model = models.resnet18(pretrained=True) #loading the pretrained model\n",
    "for param in model.parameters(): #freezing the layers\n",
    "    param.requires_grad = False\n",
    "    \n",
    "#change the last FC layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features,num_classes)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "____________________\n",
      "Train Loss: 0.6662956937666862, Acc: 59.42622950819672\n",
      "Val Loss: 0.5916550606489182, Acc: 71.24183006535948\n",
      "Epoch 1\n",
      "____________________\n",
      "Train Loss: 0.5818979557483427, Acc: 70.90163934426229\n",
      "Val Loss: 0.5042338505387306, Acc: 75.81699346405229\n",
      "Epoch 2\n",
      "____________________\n",
      "Train Loss: 0.538584554387677, Acc: 73.77049180327869\n",
      "Val Loss: 0.45520116910338404, Acc: 83.66013071895425\n",
      "Epoch 3\n",
      "____________________\n",
      "Train Loss: 0.47540173126805213, Acc: 81.55737704918033\n",
      "Val Loss: 0.4096618965268135, Acc: 81.69934640522875\n",
      "Epoch 4\n",
      "____________________\n",
      "Train Loss: 0.4798110761950093, Acc: 79.50819672131148\n",
      "Val Loss: 0.3793720968067646, Acc: 84.9673202614379\n",
      "Epoch 5\n",
      "____________________\n",
      "Train Loss: 0.43151563790536696, Acc: 84.01639344262296\n",
      "Val Loss: 0.3622742109000683, Acc: 87.58169934640523\n",
      "Epoch 6\n",
      "____________________\n",
      "Train Loss: 0.41332523092146845, Acc: 87.29508196721312\n",
      "Val Loss: 0.3142951488494873, Acc: 90.84967320261438\n",
      "Epoch 7\n",
      "____________________\n",
      "Train Loss: 0.3856493926817371, Acc: 87.70491803278688\n",
      "Val Loss: 0.337588482350111, Acc: 90.19607843137256\n",
      "Epoch 8\n",
      "____________________\n",
      "Train Loss: 0.41277823428953847, Acc: 81.9672131147541\n",
      "Val Loss: 0.3194264631718397, Acc: 88.23529411764706\n",
      "Epoch 9\n",
      "____________________\n",
      "Train Loss: 0.3871748452225039, Acc: 86.47540983606558\n",
      "Val Loss: 0.3159269496798515, Acc: 92.15686274509804\n",
      "Epoch 10\n",
      "____________________\n",
      "Train Loss: 0.3871007441513, Acc: 86.88524590163935\n",
      "Val Loss: 0.32508994042873385, Acc: 89.54248366013071\n",
      "Epoch 11\n",
      "____________________\n",
      "Train Loss: 0.37916399009766116, Acc: 87.70491803278688\n",
      "Val Loss: 0.32438303530216217, Acc: 90.84967320261438\n",
      "Epoch 12\n",
      "____________________\n",
      "Train Loss: 0.3726755981483767, Acc: 87.29508196721312\n",
      "Val Loss: 0.30318469889461996, Acc: 88.88888888888889\n",
      "Epoch 13\n",
      "____________________\n",
      "Train Loss: 0.3982557048720698, Acc: 86.47540983606558\n",
      "Val Loss: 0.31625050976872443, Acc: 90.84967320261438\n",
      "Epoch 14\n",
      "____________________\n",
      "Train Loss: 0.3951059430837631, Acc: 84.8360655737705\n",
      "Val Loss: 0.3059809826314449, Acc: 88.88888888888889\n",
      "Epoch 15\n",
      "____________________\n",
      "Train Loss: 0.3793102872948493, Acc: 87.29508196721312\n",
      "Val Loss: 0.3107029512524605, Acc: 89.54248366013071\n",
      "Epoch 16\n",
      "____________________\n",
      "Train Loss: 0.40099758005911307, Acc: 86.88524590163935\n",
      "Val Loss: 0.3058407068252563, Acc: 90.19607843137256\n",
      "Epoch 17\n",
      "____________________\n",
      "Train Loss: 0.39087064948774153, Acc: 86.47540983606558\n",
      "Val Loss: 0.30312274768948555, Acc: 91.50326797385621\n",
      "Epoch 18\n",
      "____________________\n",
      "Train Loss: 0.3850404197169888, Acc: 86.88524590163935\n",
      "Val Loss: 0.30394291281700136, Acc: 91.50326797385621\n",
      "Epoch 19\n",
      "____________________\n",
      "Train Loss: 0.3697719747020352, Acc: 88.11475409836065\n",
      "Val Loss: 0.30977909117937086, Acc: 90.19607843137256\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "#learning schedulers\n",
    "lr_scheduler1 = lr_scheduler.StepLR(optimiser,step_size=7,gamma=0.1)\n",
    "#every 7 epochs, learning rate is multiplied by 0.1\n",
    "\n",
    "trainedModel = trainModel1(model,loss_func,optimiser,lr_scheduler1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "______________\n",
      "Epoch 0, Training Loss 0.3824312090873718,Training Acc87.70491803278688\n",
      "Epoch 0, Val Loss 0.3288092613220215,Val Acc89.54248366013071\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 1/19\n",
      "______________\n",
      "Epoch 1, Training Loss 0.389901727437973,Training Acc84.42622950819673\n",
      "Epoch 1, Val Loss 0.2831222116947174,Val Acc91.50326797385621\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 2/19\n",
      "______________\n",
      "Epoch 2, Training Loss 0.40261343121528625,Training Acc84.8360655737705\n",
      "Epoch 2, Val Loss 0.2788170278072357,Val Acc92.81045751633987\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 3/19\n",
      "______________\n",
      "Epoch 3, Training Loss 0.362733393907547,Training Acc87.29508196721312\n",
      "Epoch 3, Val Loss 0.2596299946308136,Val Acc92.81045751633987\n",
      "Epoch 4/19\n",
      "______________\n",
      "Epoch 4, Training Loss 0.3588903546333313,Training Acc83.60655737704919\n",
      "Epoch 4, Val Loss 0.2582765817642212,Val Acc92.81045751633987\n",
      "Epoch 5/19\n",
      "______________\n",
      "Epoch 5, Training Loss 0.34403613209724426,Training Acc86.47540983606558\n",
      "Epoch 5, Val Loss 0.2808925211429596,Val Acc92.15686274509804\n",
      "Epoch 6/19\n",
      "______________\n",
      "Epoch 6, Training Loss 0.3266112804412842,Training Acc89.75409836065573\n",
      "Epoch 6, Val Loss 0.272445410490036,Val Acc91.50326797385621\n",
      "Epoch 7/19\n",
      "______________\n",
      "Epoch 7, Training Loss 0.31634268164634705,Training Acc89.34426229508196\n",
      "Epoch 7, Val Loss 0.24906013906002045,Val Acc93.4640522875817\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 8/19\n",
      "______________\n",
      "Epoch 8, Training Loss 0.31570538878440857,Training Acc89.75409836065573\n",
      "Epoch 8, Val Loss 0.24408093094825745,Val Acc93.4640522875817\n",
      "Epoch 9/19\n",
      "______________\n",
      "Epoch 9, Training Loss 0.3663523495197296,Training Acc85.65573770491804\n",
      "Epoch 9, Val Loss 0.23850257694721222,Val Acc91.50326797385621\n",
      "Epoch 10/19\n",
      "______________\n",
      "Epoch 10, Training Loss 0.33367854356765747,Training Acc88.93442622950819\n",
      "Epoch 10, Val Loss 0.24818681180477142,Val Acc93.4640522875817\n",
      "Epoch 11/19\n",
      "______________\n",
      "Epoch 11, Training Loss 0.3637336790561676,Training Acc87.70491803278688\n",
      "Epoch 11, Val Loss 0.23333251476287842,Val Acc92.81045751633987\n",
      "Epoch 12/19\n",
      "______________\n",
      "Epoch 12, Training Loss 0.29678085446357727,Training Acc91.39344262295081\n",
      "Epoch 12, Val Loss 0.24349074065685272,Val Acc92.81045751633987\n",
      "Epoch 13/19\n",
      "______________\n",
      "Epoch 13, Training Loss 0.3442508578300476,Training Acc87.29508196721312\n",
      "Epoch 13, Val Loss 0.2363218367099762,Val Acc94.11764705882354\n",
      "Model at epoch {epoch} saved\n",
      "Epoch 14/19\n",
      "______________\n",
      "Epoch 14, Training Loss 0.33883437514305115,Training Acc85.65573770491804\n",
      "Epoch 14, Val Loss 0.2450920194387436,Val Acc91.50326797385621\n",
      "Epoch 15/19\n",
      "______________\n",
      "Epoch 15, Training Loss 0.3451644480228424,Training Acc87.29508196721312\n",
      "Epoch 15, Val Loss 0.2558378279209137,Val Acc94.11764705882354\n",
      "Epoch 16/19\n",
      "______________\n",
      "Epoch 16, Training Loss 0.34239786863327026,Training Acc87.29508196721312\n",
      "Epoch 16, Val Loss 0.2466694861650467,Val Acc93.4640522875817\n",
      "Epoch 17/19\n",
      "______________\n",
      "Epoch 17, Training Loss 0.3187457323074341,Training Acc88.93442622950819\n",
      "Epoch 17, Val Loss 0.22696636617183685,Val Acc91.50326797385621\n",
      "Epoch 18/19\n",
      "______________\n",
      "Epoch 18, Training Loss 0.3255317509174347,Training Acc85.65573770491804\n",
      "Epoch 18, Val Loss 0.28672921657562256,Val Acc93.4640522875817\n",
      "Epoch 19/19\n",
      "______________\n",
      "Epoch 19, Training Loss 0.29620957374572754,Training Acc87.70491803278688\n",
      "Epoch 19, Val Loss 0.24341003596782684,Val Acc94.11764705882354\n",
      "Loading the best model state\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "#learning schedulers\n",
    "lr_scheduler1 = lr_scheduler.StepLR(optimiser,step_size=7,gamma=0.1)\n",
    "#every 7 epochs, learning rate is multiplied by 0.1\n",
    "\n",
    "trainedModel = trainModel(model,loss_func,optimiser,lr_scheduler1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.3641 Acc: 0.8361\n",
      "val Loss: 0.2340 Acc: 0.9216\n",
      "\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.3619 Acc: 0.8279\n",
      "val Loss: 0.2441 Acc: 0.9346\n",
      "\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.3117 Acc: 0.8811\n",
      "val Loss: 0.2292 Acc: 0.9216\n",
      "\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.3604 Acc: 0.8443\n",
      "val Loss: 0.2339 Acc: 0.9412\n",
      "\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.3387 Acc: 0.8443\n",
      "val Loss: 0.2298 Acc: 0.9477\n",
      "\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.3805 Acc: 0.8361\n",
      "val Loss: 0.2266 Acc: 0.9412\n",
      "\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.3130 Acc: 0.9016\n",
      "val Loss: 0.2269 Acc: 0.9346\n",
      "\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.2768 Acc: 0.9180\n",
      "val Loss: 0.2249 Acc: 0.9412\n",
      "\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.2987 Acc: 0.8852\n",
      "val Loss: 0.2237 Acc: 0.9477\n",
      "\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.3181 Acc: 0.8730\n",
      "val Loss: 0.2181 Acc: 0.9346\n",
      "\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.2964 Acc: 0.8811\n",
      "val Loss: 0.2221 Acc: 0.9412\n",
      "\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.2911 Acc: 0.8893\n",
      "val Loss: 0.2201 Acc: 0.9150\n",
      "\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.2943 Acc: 0.9098\n",
      "val Loss: 0.2380 Acc: 0.9346\n",
      "\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.3171 Acc: 0.8525\n",
      "val Loss: 0.2353 Acc: 0.9412\n",
      "\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.3142 Acc: 0.8770\n",
      "val Loss: 0.2228 Acc: 0.9412\n",
      "\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.3150 Acc: 0.8689\n",
      "val Loss: 0.2194 Acc: 0.9477\n",
      "\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.3003 Acc: 0.8893\n",
      "val Loss: 0.2128 Acc: 0.9412\n",
      "\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.3052 Acc: 0.8975\n",
      "val Loss: 0.2183 Acc: 0.9477\n",
      "\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.3135 Acc: 0.8811\n",
      "val Loss: 0.2226 Acc: 0.9281\n",
      "\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.2923 Acc: 0.9057\n",
      "val Loss: 0.2207 Acc: 0.9412\n",
      "\n",
      "Training complete in 1m 1s\n",
      "Best val Acc: 0.947712\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=0.001)\n",
    "#learning schedulers\n",
    "lr_scheduler1 = lr_scheduler.StepLR(optimiser,step_size=7,gamma=0.1)\n",
    "#every 7 epochs, learning rate is multiplied by 0.1\n",
    "\n",
    "trainedModel = trainModel2(model,loss_func,optimiser,lr_scheduler1,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
