{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction - Manually\n",
    "#Gradients Computation - Autograd\n",
    "#Loss Comutation - Manually\n",
    "#Parameter Updates - Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "y = torch.tensor([2,4,6,8],dtype=torch.float32) #f = 2*x is the function we need to model\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)  #weights initialised to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y,y_):\n",
    "    return ((y_ - y)**2).mean()\n",
    "\n",
    "def gradient(los):\n",
    "    los.backward()\n",
    "    g = w.grad.item()\n",
    "    w.grad.zero_()\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6., 8.])\n",
      "tensor(30., grad_fn=<MeanBackward0>)\n",
      "-30.0\n"
     ]
    }
   ],
   "source": [
    "y_ = forward(x)\n",
    "print(y)\n",
    "\n",
    "l = loss(y,y_)\n",
    "print(l)\n",
    "\n",
    "g = gradient(l)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training f(5):  0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"prediction before training f(5): \",forward(5).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch 0, Loss=30.0, Weights=0.3\n",
      "At Epoch 10, Loss=1.1627856492996216, Weights=1.67\n",
      "At Epoch 20, Loss=0.0450688973069191, Weights=1.93\n",
      "At Epoch 30, Loss=0.0017468547448515892, Weights=1.99\n",
      "At Epoch 40, Loss=6.770494655938819e-05, Weights=2.0\n",
      "At Epoch 50, Loss=2.6243997126584873e-06, Weights=2.0\n",
      "At Epoch 60, Loss=1.0175587306093803e-07, Weights=2.0\n",
      "At Epoch 70, Loss=3.9741685498029256e-09, Weights=2.0\n",
      "At Epoch 80, Loss=1.4670220593870908e-10, Weights=2.0\n",
      "At Epoch 90, Loss=5.076827847005916e-12, Weights=2.0\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "alpha = 0.01 #learning rate\n",
    "n_iters = 100\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32,requires_grad=True)  #weights initialised to zero\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #forward pass\n",
    "    y_ = forward(x)\n",
    "    \n",
    "    l = loss(y,y_)\n",
    "    \n",
    "    #backward pass\n",
    "    l.backward()\n",
    "\n",
    "    ##the following implementation will lead to error\n",
    "    #with torch.no_grad():\n",
    "        ##update weights\n",
    "        #w = w - alpha* w.grad #this should not be part of gradient computation graph\n",
    "    \n",
    "    #correct implementation\n",
    "    with torch.no_grad():\n",
    "        #update weights\n",
    "        w -= alpha* w.grad #this operation should not be part of gradient computation graph\n",
    "\n",
    "    #w.grad.zero_ #this is wrong, will not clear the gradient and will lead to error\n",
    "\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if epoch%10 == 0 :\n",
    "        print(f\"At Epoch {epoch}, Loss={l}, Weights={w:.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training f(5):  9.999998092651367\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"prediction after training f(5): \",forward(5).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
