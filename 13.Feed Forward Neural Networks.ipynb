{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST digit recognition, classification problem\n",
    "# Classifying image of hand written digits into numbers 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader, Data Transforms, Batches\n",
    "# Design the model (input size, output size, hidden layers, forward)\n",
    "#       Multi layer neural network, hidden layer activation function, final layer activation function\n",
    "#       loss function, optimiser\n",
    "# Training Loop (batch training)\n",
    "#       Forward Pass\n",
    "#       Backward Pass\n",
    "#       Update weights\n",
    "# Model Evaluation\n",
    "#       Test Accuracy\n",
    "# Plots\n",
    "#       Accuracy history, Loss history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Device configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hyperparameters\n",
    "input_size = 784 #28 x 28 images\n",
    "hidden_size1 = 500\n",
    "hidden_size2 = 124\n",
    "num_classes = 10 #number of output classes\n",
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The following model performed very poorly\n",
    "# #### Hyperparameters\n",
    "# input_size = 784 #28 x 28 images\n",
    "# hidden_size1 = 124\n",
    "# hidden_size2 = 64\n",
    "# num_classes = 10 #number of output classes\n",
    "# num_epochs = 20\n",
    "# batch_size = 100\n",
    "# learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst = torchvision.datasets.MNIST(root='/data',train=True,transform=transforms.ToTensor(),download=True)\n",
    "test_dst = torchvision.datasets.MNIST(root='/data',train=False,transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dst,batch_size=batch_size,shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dst,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Have a look at first batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_loader)\n",
    "batch_data = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,labels = batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 28, 28])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Visualise Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(inputs[i][0],cmap='gray') #accessing the first channel\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_data(Dataset):\n",
    "    def __init__(self):\n",
    "        #\n",
    "\n",
    "        mnist_dst = torchvision.datasets.MNIST(root='/data',train=true)\n",
    "    def __get_item__(self,index):\n",
    "        #\n",
    "        8\n",
    "    def __len__(self):\n",
    "        #\n",
    "        8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet_HW(nn.Module):\n",
    "    def __init__(self,input_features,hidden_size1,hidden_size2,output_features):\n",
    "        super(NeuralNet_HW,self).__init__()\n",
    "        self.lin1 = nn.Linear(input_features,hidden_size1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(hidden_size1,hidden_size2)\n",
    "        self.lin3 = nn.Linear(hidden_size2,output_features) \n",
    "        self.softmax = nn.Softmax() #remove this if we are using cross entropy loss\n",
    "    def forward(self,x):\n",
    "        out = self.lin1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lin3(out)\n",
    "        return out\n",
    "        #out = self.softmax(out) we dont apply this, as we are using cross entropy loss, it applies softmax for as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet_HW(input_size,hidden_size1,hidden_size2,num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    " # model.summary()  check this out later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet_HW(\n",
       "  (lin1): Linear(in_features=784, out_features=500, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=500, out_features=124, bias=True)\n",
       "  (lin3): Linear(in_features=124, out_features=10, bias=True)\n",
       "  (softmax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function and Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, step 200 / 600, loss = 0.1657\n",
      "epoch 1 / 20, step 400 / 600, loss = 0.2221\n",
      "epoch 1 / 20, step 600 / 600, loss = 0.0582\n",
      "epoch 2 / 20, step 200 / 600, loss = 0.1392\n",
      "epoch 2 / 20, step 400 / 600, loss = 0.1473\n",
      "epoch 2 / 20, step 600 / 600, loss = 0.0286\n",
      "epoch 3 / 20, step 200 / 600, loss = 0.0664\n",
      "epoch 3 / 20, step 400 / 600, loss = 0.0846\n",
      "epoch 3 / 20, step 600 / 600, loss = 0.0485\n",
      "epoch 4 / 20, step 200 / 600, loss = 0.0093\n",
      "epoch 4 / 20, step 400 / 600, loss = 0.0427\n",
      "epoch 4 / 20, step 600 / 600, loss = 0.0347\n",
      "epoch 5 / 20, step 200 / 600, loss = 0.0221\n",
      "epoch 5 / 20, step 400 / 600, loss = 0.0053\n",
      "epoch 5 / 20, step 600 / 600, loss = 0.0184\n",
      "epoch 6 / 20, step 200 / 600, loss = 0.0684\n",
      "epoch 6 / 20, step 400 / 600, loss = 0.0675\n",
      "epoch 6 / 20, step 600 / 600, loss = 0.0211\n",
      "epoch 7 / 20, step 200 / 600, loss = 0.0387\n",
      "epoch 7 / 20, step 400 / 600, loss = 0.0297\n",
      "epoch 7 / 20, step 600 / 600, loss = 0.0257\n",
      "epoch 8 / 20, step 200 / 600, loss = 0.0242\n",
      "epoch 8 / 20, step 400 / 600, loss = 0.0026\n",
      "epoch 8 / 20, step 600 / 600, loss = 0.0035\n",
      "epoch 9 / 20, step 200 / 600, loss = 0.0201\n",
      "epoch 9 / 20, step 400 / 600, loss = 0.0202\n",
      "epoch 9 / 20, step 600 / 600, loss = 0.0254\n",
      "epoch 10 / 20, step 200 / 600, loss = 0.0054\n",
      "epoch 10 / 20, step 400 / 600, loss = 0.0808\n",
      "epoch 10 / 20, step 600 / 600, loss = 0.0662\n",
      "epoch 11 / 20, step 200 / 600, loss = 0.0032\n",
      "epoch 11 / 20, step 400 / 600, loss = 0.0021\n",
      "epoch 11 / 20, step 600 / 600, loss = 0.0003\n",
      "epoch 12 / 20, step 200 / 600, loss = 0.0016\n",
      "epoch 12 / 20, step 400 / 600, loss = 0.0012\n",
      "epoch 12 / 20, step 600 / 600, loss = 0.0143\n",
      "epoch 13 / 20, step 200 / 600, loss = 0.0427\n",
      "epoch 13 / 20, step 400 / 600, loss = 0.0005\n",
      "epoch 13 / 20, step 600 / 600, loss = 0.0419\n",
      "epoch 14 / 20, step 200 / 600, loss = 0.0004\n",
      "epoch 14 / 20, step 400 / 600, loss = 0.0059\n",
      "epoch 14 / 20, step 600 / 600, loss = 0.0509\n",
      "epoch 15 / 20, step 200 / 600, loss = 0.0197\n",
      "epoch 15 / 20, step 400 / 600, loss = 0.0013\n",
      "epoch 15 / 20, step 600 / 600, loss = 0.0025\n",
      "epoch 16 / 20, step 200 / 600, loss = 0.0034\n",
      "epoch 16 / 20, step 400 / 600, loss = 0.0019\n",
      "epoch 16 / 20, step 600 / 600, loss = 0.0049\n",
      "epoch 17 / 20, step 200 / 600, loss = 0.0002\n",
      "epoch 17 / 20, step 400 / 600, loss = 0.0004\n",
      "epoch 17 / 20, step 600 / 600, loss = 0.0001\n",
      "epoch 18 / 20, step 200 / 600, loss = 0.0016\n",
      "epoch 18 / 20, step 400 / 600, loss = 0.0017\n",
      "epoch 18 / 20, step 600 / 600, loss = 0.0002\n",
      "epoch 19 / 20, step 200 / 600, loss = 0.0017\n",
      "epoch 19 / 20, step 400 / 600, loss = 0.0006\n",
      "epoch 19 / 20, step 600 / 600, loss = 0.0004\n",
      "epoch 20 / 20, step 200 / 600, loss = 0.0001\n",
      "epoch 20 / 20, step 400 / 600, loss = 0.0041\n",
      "epoch 20 / 20, step 600 / 600, loss = 0.0000\n"
     ]
    }
   ],
   "source": [
    "num_steps_per_epoch = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs) : #looping over epochs\n",
    "    for i,(inputs,labels) in enumerate(train_loader): #looping over batches\n",
    "\n",
    "        #Flatten the images\n",
    "\n",
    "        #inputs have shape 100,1,28,28, we need to change this to 100,784 \n",
    "        inputs = inputs.view(-1,28*28).to(device) #copy the to gpu, as we do training on the gpu \n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #forward pass\n",
    "        y_preds = model(inputs)\n",
    "        # print(y_preds.shape), should be n_samples x n_classes\n",
    "        loss = loss_func(y_preds,labels)\n",
    "\n",
    "        #backward pass\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #weight updation\n",
    "        optimiser.step()\n",
    "\n",
    "        if (i+1)%200 == 0:\n",
    "            print(f\"epoch {epoch+1} / {num_epochs}, step {i+1} / {num_steps_per_epoch}, loss = {loss.item():.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model = 98.10%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct_pred = 0\n",
    "    n_samples = 0\n",
    "    for i,(inputs,labels) in enumerate(test_loader):\n",
    "        inputs = inputs.view(-1,28*28).to(device) #copy to gpu\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _,preds = torch.max(outputs,1) #returns value snad index, inndex is the class label\n",
    "\n",
    "        n_samples += inputs.shape[0]\n",
    "        correct_pred += torch.eq(labels,preds).sum().item()\n",
    "        \n",
    "    accuracy = (100.0*correct_pred)/n_samples\n",
    "    print(f\"Test accuracy of the model = {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model = 99.81%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct_pred = 0\n",
    "    n_samples = 0\n",
    "    for i,(inputs,labels) in enumerate(train_loader):\n",
    "        inputs = inputs.view(-1,28*28).to(device) #copy to gpu\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        _,preds = torch.max(outputs,1) #returns value snad index, inndex is the class label\n",
    "\n",
    "        n_samples += inputs.shape[0]\n",
    "        correct_pred += torch.eq(labels,preds).sum().item()\n",
    "        \n",
    "    accuracy = (100.0*correct_pred)/n_samples\n",
    "    print(f\"Test accuracy of the model = {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
