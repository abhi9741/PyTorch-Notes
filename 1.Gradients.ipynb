{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4046, -0.3089,  0.0404])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8316,  1.8105,  0.6676], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)  #if we want to calculate gradients in the future, we need to set requires_grad=True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1684, 3.8105, 2.6676], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x+2\n",
    "y\n",
    "#AddBackward0 - grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.7302, 29.0403, 14.2324], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = y*y*2\n",
    "print(c)\n",
    "#MulBackward0 -  grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(15.3343, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "k = c.mean()\n",
    "print(k)\n",
    "#MeanBackward0 - grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to calculate the gradients, \n",
    "\n",
    "#dk/dx\n",
    "k.backward() #will calculate gradient of K, with respect to x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5578, 5.0807, 3.5568])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4653,  0.8673, -3.2148,  0.0388, -0.0211], requires_grad=True)\n",
      "tensor(8.0500, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.2277,  2.2938, -0.9718,  1.6310,  1.5831])\n"
     ]
    }
   ],
   "source": [
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2470, -0.1975, -2.1230,  2.1002,  0.0234])\n",
      "tensor(11.6877)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch\\Gradients.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000014?line=0'>1</a>\u001b[0m z\u001b[39m.\u001b[39;49mbackward() \u001b[39m#dz/dx\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000014?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "z.backward() #dz/dx\n",
    "print(x.grad)\n",
    "#we must specify requires grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0429, -0.5521, -0.8023, -1.5637, -0.0280], requires_grad=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch\\Gradients.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000015?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m x\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000015?line=3'>4</a>\u001b[0m z \u001b[39m=\u001b[39m y\u001b[39m*\u001b[39my\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m#not a ascalar, a vector\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000015?line=5'>6</a>\u001b[0m z\u001b[39m.\u001b[39;49mbackward() \u001b[39m#dz/dx\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch/Gradients.ipynb#ch0000015?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=161'>162</a>\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=162'>163</a>\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=164'>165</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=165'>166</a>\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=166'>167</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:67\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=64'>65</a>\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m     <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=65'>66</a>\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=66'>67</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=67'>68</a>\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[0;32m     <a href='file:///c%3A/Users/Abhinav%20Reddy%20Nimma/miniconda3/envs/pytorch/lib/site-packages/torch/autograd/__init__.py?line=68'>69</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2 #not a ascalar, a vector\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1534, -0.0679,  1.5682], requires_grad=True)\n",
      "tensor([0.7386, 7.7285, 0.0143])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2 #not a ascalar, a vector\n",
    "\n",
    "v = torch.tensor([0.1,1.0,0.001],dtype=torch.float32)\n",
    "z.backward(v) #vector jacobian operation, we must give it the vector if it is not a scalar\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1159,  0.7055,  1.7181], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#if we do not want gradients for certain operations or steps (like updating weights etc)\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#x.detach() #creates a new tensor that doesnot require gradient\n",
    "#with torch.no_grad() :\n",
    "    #operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7212, -0.7527, -0.3640], requires_grad=True)\n",
      "tensor([ 0.7212, -0.7527, -0.3640])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4923, -1.7609,  0.2414], requires_grad=True)\n",
      "tensor([-1.4923, -1.7609,  0.2414])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach() #new tensor with same values but that doesnt require the gradients\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3289, -0.2471,  1.9028], requires_grad=True)\n",
      "tensor([2.3289, 1.7529, 3.9028])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "print(y) #will not have grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3820,  1.4834, -1.2198], requires_grad=True)\n",
      "tensor([2.3820, 3.4834, 0.7802], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y) #will have grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations\n",
    "\n",
    "#our gradients are clearly wrong, they are getting accumulated\n",
    "#to prevent this we must empty the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations\n",
    "\n",
    "    weights.grad.zero_()\n",
    "\n",
    "\n",
    "#to prevent gradient accumulation, we must empty the gradients after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights,lr=0.01)\n",
    "optimizer.step()\n",
    "\n",
    "#before doing the next iteration, we must call the zero grad function on the optimiser\n",
    "optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d55778a3033b6a2514b9fd3a4f7598f5122295c8ca77ad3bd42cbb3f7282272d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
