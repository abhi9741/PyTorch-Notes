{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction - PyTorch Model\n",
    "#Gradients Computation - Autograd\n",
    "#Loss Comutation - PyTorch Loss\n",
    "#Parameter Updates - PyTorch Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "# x = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
    "# y = torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "#The inputs and outputs should be 2D array, where\n",
    "#Number of rows =  number of samples and Number of columns = number of features\n",
    "\n",
    "x = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "n_samples,n_features = x.shape\n",
    "print(n_samples,n_features)\n",
    "\n",
    "#w = torch.tensor(0.0,dtype=torch.float32,requires_grad=True) \n",
    "#Do not need our weights anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def forward(x):\n",
    "    #return w*x\n",
    "\n",
    "#No manual forward/predicion function\n",
    "#No manual loss function\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size,output_size) #weights are randomly initialised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction before training f(5):  2.058760643005371\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_test = torch.tensor([5],dtype=torch.float32)\n",
    "\n",
    "print(\"prediction before training f(5): \",model(x_test).item()) #input must be a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch 0, Loss=18.82791519165039, Weights=0.644\n",
      "At Epoch 10, Loss=0.5271768569946289, Weights=1.64\n",
      "At Epoch 20, Loss=0.05136220157146454, Weights=1.81\n",
      "At Epoch 30, Loss=0.036856040358543396, Weights=1.84\n",
      "At Epoch 40, Loss=0.03441288322210312, Weights=1.85\n",
      "At Epoch 50, Loss=0.03240213915705681, Weights=1.85\n",
      "At Epoch 60, Loss=0.03051598183810711, Weights=1.86\n",
      "At Epoch 70, Loss=0.02873978577554226, Weights=1.86\n",
      "At Epoch 80, Loss=0.027066994458436966, Weights=1.86\n",
      "At Epoch 90, Loss=0.02549152821302414, Weights=1.87\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "learning_rate = 0.01 #learning rate\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=learning_rate) #Stocastic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #forward pass\n",
    "    y_ = model(x)\n",
    "    \n",
    "    l = loss(y,y_)\n",
    "    \n",
    "    #backward pass\n",
    "    l.backward() #gradient calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #update weights\n",
    "        optimiser.step()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "\n",
    "    if epoch%10 == 0 :\n",
    "        w,b = model.parameters()\n",
    "        print(f\"At Epoch {epoch}, Loss={l}, Weights={w[0][0].item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training f(5):  9.734369277954102\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction after training f(5): \",model(x_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearReg(nn.Module): #deriving this class from nn \n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearReg,self).__init__() #super() gives you access to methods in a superclass from the subclass that inherits from it.\n",
    "        #define layers\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1],[2],[3],[4]],dtype=torch.float32)\n",
    "y = torch.tensor([[2],[4],[6],[8]],dtype=torch.float32)\n",
    "\n",
    "[n_samples,n_features] = x.shape\n",
    "print(n_samples,n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At Epoch 0, Loss=34.40201187133789, Weights=0.511\n",
      "At Epoch 10, Loss=0.9065592885017395, Weights=1.85\n",
      "At Epoch 20, Loss=0.03899206966161728, Weights=2.06\n",
      "At Epoch 30, Loss=0.015641838312149048, Weights=2.1\n",
      "At Epoch 40, Loss=0.01418598834425211, Weights=2.1\n",
      "At Epoch 50, Loss=0.01334618404507637, Weights=2.1\n",
      "At Epoch 60, Loss=0.012568985112011433, Weights=2.09\n",
      "At Epoch 70, Loss=0.011837398633360863, Weights=2.09\n",
      "At Epoch 80, Loss=0.011148425750434399, Weights=2.09\n",
      "At Epoch 90, Loss=0.010499552823603153, Weights=2.09\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "learning_rate = 0.01 #learning rate\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "model = LinearReg(input_dim=n_features,output_dim=n_features)\n",
    "\n",
    "\n",
    "optimiser = torch.optim.SGD(model.parameters(),lr=learning_rate) #Stocastic Gradient Descent\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #forward pass\n",
    "    y_ = model(x)\n",
    "    \n",
    "    l = loss(y,y_)\n",
    "    \n",
    "    #backward pass\n",
    "    l.backward() #gradient calculation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        #update weights\n",
    "        optimiser.step()\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "\n",
    "    if epoch%10 == 0 :\n",
    "        w,b = model.parameters()\n",
    "        print(f\"At Epoch {epoch}, Loss={l}, Weights={w[0][0].item():.3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction after training f(5):  10.170477867126465\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction after training f(5): \",model(x_test).item())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d55778a3033b6a2514b9fd3a4f7598f5122295c8ca77ad3bd42cbb3f7282272d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
