{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2510, 0.4039, 0.0782])\n",
      "tensor([ 1.6417, -1.1004, -1.6507])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3) # 3 random numbers in te range [0,1]\n",
    "print(x)\n",
    "\n",
    "x = torch.randn(3) # 3 random numbers drawn from the standard normal distribution (mean value is 0, variance is 1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6734, 0.7100, 0.0370])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5886,  0.3995, -0.5213], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)  #if we want to calculate gradients in the future, we need to set requires_grad=True\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5886, 2.3995, 1.4787], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = x+2\n",
    "y\n",
    "#AddBackward0 - grad function, that tracks all the steps, in order to calculate the grads later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([13.4022, 11.5148,  4.3733], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = y*y*2\n",
    "print(c)\n",
    "#MulBackward0 -  grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.7634, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "k = c.mean() #every operation you make on \"requires_grad\" variable is tracked\n",
    "print(k)\n",
    "#MeanBackward0 - grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we want to calculate the gradients, \n",
    "#dk/dx\n",
    "\n",
    "k.backward() #will calculate gradient of K, with respect to x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.4515, 3.1993, 1.9716])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2970, -0.9738,  0.1661, -0.4632, -2.2943], requires_grad=True)\n",
      "tensor(3.4751, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5624,  0.8210,  1.7329,  1.2294, -0.2354])\n"
     ]
    }
   ],
   "source": [
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2994, -0.0782, -0.4953,  0.2648,  0.9002])\n",
      "tensor(8.9561)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5) #requires gradient is false by default\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\1.Gradients.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000011?line=0'>1</a>\u001b[0m z\u001b[39m.\u001b[39;49mbackward() \u001b[39m#dz/dx\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000011?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> 154\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    155\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    156\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "z.backward() #dz/dx\n",
    "print(x.grad)\n",
    "#we must specify requires grad if we want to calculate gradients using .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.2614,  0.9733,  1.7545,  1.2994,  0.4667], requires_grad=True)\n",
      "tensor([0.7386, 2.9733, 3.7545, 3.2994, 2.4667], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\1.Gradients.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000026?line=2'>3</a>\u001b[0m y \u001b[39m=\u001b[39m x\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000026?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(y)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000026?line=5'>6</a>\u001b[0m y\u001b[39m.\u001b[39;49mbackward() \u001b[39m#dz/dx\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000026?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mgrad)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:150\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    146\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    147\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    149\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 150\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_)\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:51\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y) #Y is a vector and not a scalar, not a single value output, gradients cannot be calculated\n",
    "\n",
    "y.backward() #dz/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2090,  1.6224,  0.1300, -0.0889,  0.4065], requires_grad=True)\n",
      "tensor([1.7910, 3.6224, 2.1300, 1.9111, 2.4065], grad_fn=<AddBackward0>)\n",
      "tensor([0.2000, 0.2000, 0.2000, 0.2000, 0.2000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y) #Y is a vector and not a scalar, not a single value output, gradients cannot be calculated\n",
    "y = y.mean() #y is a scalar quantity, single value\n",
    "\n",
    "y.backward() #dz/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2022, -0.2681, -0.6680,  0.8448, -0.1477], requires_grad=True)\n",
      "tensor([1.7978, 1.7319, 1.3320, 2.8448, 1.8523], grad_fn=<AddBackward0>)\n",
      "tensor([ 6.4641,  5.9987,  3.5486, 16.1862,  6.8620], grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abhinav Reddy Nimma\\Desktop\\Artificial Intelligence\\PyTorch-Notes\\1.Gradients.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000012?line=4'>5</a>\u001b[0m z \u001b[39m=\u001b[39m y\u001b[39m*\u001b[39my\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m#not a scalar, a vector\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000012?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(z)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abhinav%20Reddy%20Nimma/Desktop/Artificial%20Intelligence/PyTorch-Notes/1.Gradients.ipynb#ch0000012?line=7'>8</a>\u001b[0m z\u001b[39m.\u001b[39;49mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    300\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    301\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    306\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 307\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:150\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    146\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    147\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    149\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 150\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_)\n\u001b[0;32m    151\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\torch\\autograd\\__init__.py:51\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 51\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39mones_like(out, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format))\n\u001b[0;32m     53\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y)\n",
    "z = y*y*2 #not a scalar, a vector\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7664, -0.6811,  0.4952, -1.4559, -0.1810], requires_grad=True)\n",
      "tensor([2.7664, 1.3189, 2.4952, 0.5441, 1.8190], grad_fn=<AddBackward0>)\n",
      "tensor([15.3062,  3.4792, 12.4522,  0.5921,  6.6177], grad_fn=<MulBackward0>)\n",
      "tensor([2.2131, 1.0551, 1.9962, 0.4353, 1.4552])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(5,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y)\n",
    "z = y*y*2 #not a scalar, a vector\n",
    "print(z)\n",
    "z = z.mean() #Scalar\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1860, -0.1098, -1.7606], requires_grad=True)\n",
      "tensor([8.7439e-01, 7.5608e+00, 9.5773e-04])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "z = y*y*2 #not a ascalar, a vector\n",
    "\n",
    "v = torch.tensor([0.1,1.0,0.001],dtype=torch.float32)\n",
    "z.backward(v) #vector jacobian operation, we must give it the vector if it is not a scalar, read more about this and add some stuff here\n",
    "print(x.grad)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.1159,  0.7055,  1.7181], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#if we do not want gradients for certain operations or steps (like updating weights etc) #no gradient #no grad\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#x.detach() #creates a new tensor that doesnot require gradient\n",
    "#with torch.no_grad() :\n",
    "    #operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7212, -0.7527, -0.3640], requires_grad=True)\n",
      "tensor([ 0.7212, -0.7527, -0.3640])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False) #_ inplace\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4923, -1.7609,  0.2414], requires_grad=True)\n",
      "tensor([-1.4923, -1.7609,  0.2414])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach() #new tensor with same values but that doesnt require the gradients\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6017, -0.1214, -0.2886], requires_grad=True)\n",
      "tensor([1.3983, 1.8786, 1.7114])\n",
      "tensor([1.3983, 1.8786, 1.7114], grad_fn=<AddBackward0>)\n",
      "tensor([3.3983, 3.8786, 3.7114])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x+2\n",
    "    \n",
    "z = x + 2\n",
    "g = y + 2\n",
    "print(y) #will not have grad function\n",
    "print(z)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4837, -0.3361,  0.0720], requires_grad=True)\n",
      "tensor([2.4837, 1.6639, 2.0720], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "print(x)\n",
    "y = x+2\n",
    "print(y) #will have grad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#important example # gradient accumulation # clear gradients # empty gradients\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(1):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(2):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations\n",
    "\n",
    "#our gradients are clearly wrong, they are getting accumulated\n",
    "#to prevent this we must empty the gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "#important example\n",
    "\n",
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    out = (weights*3).sum()\n",
    "\n",
    "    out.backward()\n",
    "    \n",
    "    print(weights.grad) #it will accumulate the values from previous operations\n",
    "\n",
    "    weights.grad.zero_()\n",
    "\n",
    "\n",
    "#to prevent gradient accumulation, we must empty the gradients after each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.ones(4,requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights,lr=0.01)\n",
    "optimizer.step()\n",
    "\n",
    "#before doing the next iteration, we must call the zero grad function on the optimiser\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83bec4cd18966703d39e3fd8209371b7574445e5ac2dfe27a80cb33e9b531167"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
